<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Capítulo 11 - Alta Disponibilidad y Resiliencia del Sistema</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>

<div class="nav-header">
    <h2>Capítulo 11: Alta Disponibilidad y Resiliencia del Sistema</h2>
    <div class="nav-buttons">
        <a href="capitulo_10.html" class="nav-btn">← Anterior</a>
        <a href="index.html" class="nav-btn">Índice</a>
        <a href="capitulo_12.html" class="nav-btn">Siguiente →</a>
    </div>
</div>

<h1>11. ALTA DISPONIBILIDAD Y RESILIENCIA DEL SISTEMA</h1>

<h2>11.1 Objetivos de Disponibilidad (SLA)</h2>

<div class="section">
    <table>
        <thead>
            <tr>
                <th>Métrica</th>
                <th>Objetivo</th>
                <th>Cálculo Anual</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Uptime</strong></td>
                <td>99.9% (Three Nines)</td>
                <td>8.76 horas downtime/año</td>
            </tr>
            <tr>
                <td><strong>RTO</strong> (Recovery Time Objective)</td>
                <td>&lt; 15 minutos</td>
                <td>Tiempo máximo de recuperación</td>
            </tr>
            <tr>
                <td><strong>RPO</strong> (Recovery Point Objective)</td>
                <td>&lt; 5 minutos</td>
                <td>Pérdida máxima de datos</td>
            </tr>
            <tr>
                <td><strong>Latencia API</strong></td>
                <td>&lt; 200ms (p95)</td>
                <td>95% de requests bajo 200ms</td>
            </tr>
            <tr>
                <td><strong>Error Rate</strong></td>
                <td>&lt; 0.1%</td>
                <td>Máximo 1 error por 1000 requests</td>
            </tr>
        </tbody>
    </table>

    <div class="note">
        <strong>99.9% Uptime significa:</strong>
        <ul>
            <li>8.76 horas de downtime permitido por año</li>
            <li>43.8 minutos por mes</li>
            <li>10.1 minutos por semana</li>
            <li>1.44 minutos por día</li>
        </ul>
    </div>
</div>

<h2>11.2 Estrategias de Alta Disponibilidad</h2>

<div class="section">
    <h3>11.2.1 Réplicas de Microservicios</h3>
    
    <ul>
        <li><strong>Mínimo 2 réplicas en producción</strong> (hasta 3 para servicios críticos)</li>
        <li><strong>Desplegadas en diferentes nodos</strong> (anti-affinity rules en K8s)</li>
        <li><strong>Health checks continuos</strong> cada 10 segundos</li>
        <li><strong>Recreación automática</strong> de pods fallidos</li>
        <li><strong>Rolling updates</strong> sin downtime (20% max surge)</li>
    </ul>

    <h3>11.2.2 Réplicas de Bases de Datos</h3>
    
    <div class="diagram">
        <div class="diagram-title">PostgreSQL Master-Replica con Failover Automático</div>
        <pre>
┌─────────────┐
│  Primary    │ ←── Patroni/Stolon (Automatic Failover)
│ (Read+Write)│
└──────┬──────┘
       │ Streaming Replication (Asíncrona)
       ├────────────┬────────────┐
       │            │            │
┌──────▼──────┐ ┌───▼──────┐ ┌──▼───────┐
│  Replica 1  │ │ Replica 2│ │ Replica 3│
│ (Read-only) │ │(Read-only│ │(Read-only│
│  Zona AZ-A  │ │ Zona AZ-B│ │ Zona AZ-C│
└─────────────┘ └──────────┘ └──────────┘

Failover Automático:
1. Primary cae
2. Patroni detecta (5 segundos)
3. Elige nueva Primary (Replica con menos lag)
4. Promoción automática (10 segundos)
5. Actualiza DNS/Endpoints
6. Total: &lt; 20 segundos de interrupción
        </pre>
    </div>

    <h3>11.2.3 Circuit Breaker Pattern</h3>
    
    <div class="diagram">
        <div class="diagram-title">Estados del Circuit Breaker</div>
        <pre>
Estado Normal (CLOSED):
  Request → Servicio ✓
  └─► Se monitorean fallos

Estado Abierto (OPEN):
  Request → Circuit Breaker → Error inmediato
  └─► NO se llama al servicio (fail-fast)
  └─► Espera 30 segundos → HALF_OPEN

Estado Semi-Abierto (HALF_OPEN):
  Request → Circuit Breaker → Intenta 1 request de prueba
  ├─► Si éxito → CLOSED (vuelve a normal)
  └─► Si falla → OPEN (espera otros 30s)
        </pre>
    </div>

    <table>
        <thead>
            <tr>
                <th>Servicio</th>
                <th>Threshold</th>
                <th>Timeout</th>
                <th>Reset Time</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>MS-Datos-Cliente</td>
                <td>50% error en 10 req</td>
                <td>5s</td>
                <td>30s</td>
            </tr>
            <tr>
                <td>MS-Pagos</td>
                <td>60% error en 20 req</td>
                <td>10s</td>
                <td>60s</td>
            </tr>
            <tr>
                <td>ACH Network (externo)</td>
                <td>40% error en 5 req</td>
                <td>15s</td>
                <td>120s</td>
            </tr>
            <tr>
                <td>MS-Notificaciones</td>
                <td>70% error en 50 req</td>
                <td>3s</td>
                <td>20s</td>
            </tr>
        </tbody>
    </table>

    <h3>11.2.4 Retry con Exponential Backoff + Jitter</h3>
    
    <pre>
Intento 1: Inmediato
  ├─ Falla → Esperar 1s + random(0-200ms)
  │
Intento 2: Después de ~1s
  ├─ Falla → Esperar 2s + random(0-400ms)
  │
Intento 3: Después de ~2s
  ├─ Falla → Esperar 4s + random(0-800ms)
  │
Intento 4: Después de ~4s
  ├─ Falla → Esperar 8s + random(0-1600ms)
  │
Intento 5: Después de ~8s
  └─ Falla → Error definitivo, ejecutar compensación

Jitter (ruido aleatorio): Previene "thundering herd"
cuando múltiples clientes reintentan simultáneamente
    </pre>

    <h3>11.2.5 Graceful Degradation</h3>
    
    <table>
        <thead>
            <tr>
                <th>Escenario</th>
                <th>Servicio Afectado</th>
                <th>Comportamiento Degradado</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>MS-Históricos caído</td>
                <td>Consulta movimientos</td>
                <td>Mostrar últimos 5 desde caché + mensaje "Datos de hace 15 min"</td>
            </tr>
            <tr>
                <td>ACH Network lento</td>
                <td>Transferencias interbancarias</td>
                <td>Mostrar "Procesando, recibirás notificación" (asíncrono)</td>
            </tr>
            <tr>
                <td>MS-Notificaciones saturado</td>
                <td>Envío de emails</td>
                <td>Queue notifications, enviar cuando se recupere</td>
            </tr>
            <tr>
                <td>Redis caído</td>
                <td>Caché de sesiones</td>
                <td>Leer directo de PostgreSQL (más lento pero funcional)</td>
            </tr>
            <tr>
                <td>Firebase caído</td>
                <td>Push notifications</td>
                <td>Fallback a SMS o solo email</td>
            </tr>
        </tbody>
    </table>
</div>

<h2>11.3 Disaster Recovery</h2>

<div class="section">
    <h3>11.3.1 Estrategia de Backup</h3>
    
    <table>
        <thead>
            <tr>
                <th>Tipo</th>
                <th>Frecuencia</th>
                <th>Retención</th>
                <th>Storage</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Full Backup</strong></td>
                <td>Semanal (Domingo 2am)</td>
                <td>3 meses</td>
                <td>S3 Glacier</td>
            </tr>
            <tr>
                <td><strong>Incremental</strong></td>
                <td>Diario (2am)</td>
                <td>30 días</td>
                <td>S3 Standard</td>
            </tr>
            <tr>
                <td><strong>Transaction Logs (WAL)</strong></td>
                <td>Continuo</td>
                <td>7 días</td>
                <td>S3 Standard</td>
            </tr>
            <tr>
                <td><strong>Snapshots</strong></td>
                <td>Cada 6 horas</td>
                <td>48 horas</td>
                <td>EBS Snapshots</td>
            </tr>
        </tbody>
    </table>

    <h3>11.3.2 Plan de Recuperación ante Desastres</h3>
    
    <table>
        <thead>
            <tr>
                <th>Escenario</th>
                <th>RTO</th>
                <th>RPO</th>
                <th>Procedimiento</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>1 microservicio caído</strong></td>
                <td>&lt;2 min</td>
                <td>0</td>
                <td>K8s restart automático del pod</td>
            </tr>
            <tr>
                <td><strong>1 nodo caído</strong></td>
                <td>&lt;5 min</td>
                <td>0</td>
                <td>K8s reschedula pods en otros nodos</td>
            </tr>
            <tr>
                <td><strong>Zona AZ completa caída</strong></td>
                <td>&lt;5 min</td>
                <td>0</td>
                <td>Failover automático a otra AZ (multi-AZ deployment)</td>
            </tr>
            <tr>
                <td><strong>Región completa caída</strong></td>
                <td>&lt;15 min</td>
                <td>&lt;5 min</td>
                <td>Failover manual a región secundaria</td>
            </tr>
            <tr>
                <td><strong>Corrupción de BD</strong></td>
                <td>&lt;30 min</td>
                <td>&lt;5 min</td>
                <td>PITR restore desde WAL logs</td>
            </tr>
            <tr>
                <td><strong>Ransomware</strong></td>
                <td>&lt;1 hora</td>
                <td>&lt;1 hora</td>
                <td>Restore desde backup offline (air-gapped)</td>
            </tr>
        </tbody>
    </table>

    <h3>11.3.3 Database PITR (Point-in-Time Recovery)</h3>
    
    <div class="diagram">
        <div class="diagram-title">Estrategia PITR PostgreSQL</div>
        <pre>
┌────────────────────────────────────────────────┐
│      POSTGRESQL PITR STRATEGY                  │
├────────────────────────────────────────────────┤
│                                                │
│  Base Backup (Full): Domingo 2am               │
│  │                                             │
│  ├─► Stored in S3 Glacier                     │
│  ├─► Compressed with pg_dump                  │
│  └─► Retention: 90 días                       │
│                                                │
│  WAL (Write-Ahead Logs): Continuo             │
│  │                                             │
│  ├─► Archived every 16MB or 5 minutes         │
│  ├─► Stored in S3 Standard                    │
│  ├─► Retention: 30 días                       │
│  └─► Enable PITR a cualquier momento          │
│                                                │
│  Procedimiento de Recuperación:                │
│  ├─► 1. Restaurar Full Backup más reciente    │
│  ├─► 2. Aplicar WAL logs hasta punto deseado  │
│  ├─► 3. Tiempo de recuperación: 15-30 min     │
│  └─► 4. Pérdida máxima de datos: 5 minutos    │
│                                                │
└────────────────────────────────────────────────┘
        </pre>
    </div>
</div>

<h2>11.4 Monitoreo y Alertas</h2>

<div class="section">
    <h3>11.4.1 Sistema de Alertas Proactivo</h3>
    
    <table>
        <thead>
            <tr>
                <th>Alerta</th>
                <th>Umbral</th>
                <th>Acción</th>
                <th>Canal</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>CPU &gt; 80%</td>
                <td>5 min consecutivos</td>
                <td>Escalar pods automáticamente</td>
                <td>Slack + Email</td>
            </tr>
            <tr>
                <td>Error Rate &gt; 1%</td>
                <td>2 minutos</td>
                <td>Investigación inmediata</td>
                <td>PagerDuty</td>
            </tr>
            <tr>
                <td>Latencia &gt; 500ms</td>
                <td>p95 por 3 min</td>
                <td>Revisar cuellos de botella</td>
                <td>Slack</td>
            </tr>
            <tr>
                <td>Disco &gt; 85%</td>
                <td>Instantáneo</td>
                <td>Limpiar logs antiguos</td>
                <td>Email</td>
            </tr>
            <tr>
                <td>Pod restart loop</td>
                <td>3 restarts en 5 min</td>
                <td>Alerta crítica</td>
                <td>PagerDuty</td>
            </tr>
            <tr>
                <td>Backup fallido</td>
                <td>Cualquier fallo</td>
                <td>Acción inmediata</td>
                <td>Email + SMS</td>
            </tr>
        </tbody>
    </table>

    <h3>11.4.2 Dashboard de Operaciones en Grafana</h3>
    
    <ul>
        <li><strong>Estado de salud</strong> de todos los microservicios</li>
        <li><strong>Métricas de negocio:</strong> Transacciones/hora, monto total</li>
        <li><strong>Latencias por endpoint:</strong> p50, p95, p99</li>
        <li><strong>Tasa de errores:</strong> Por servicio y global</li>
        <li><strong>Uso de recursos:</strong> CPU, RAM, Disco por pod</li>
        <li><strong>Salud de BD y caché:</strong> Conexiones, latencia, hit rate</li>
    </ul>
</div>

<h2>11.5 RESILIENCIA DEL SISTEMA</h2>

<div class="section">
    <p>La resiliencia es la capacidad del sistema para <strong>resistir, adaptarse y recuperarse</strong> de fallos, disrupciones y cambios en la demanda, manteniendo un nivel aceptable de servicio.</p>

    <div class="diagram">
        <div class="diagram-title">Tres Pilares de Resiliencia</div>
        <pre>
┌────────────────────────────────────────────────────────┐
│              RESILIENCIA DEL SISTEMA                   │
├────────────────────────────────────────────────────────┤
│                                                        │
│  ┌──────────────┐  ┌──────────────┐  ┌─────────────┐  │
│  │ RESISTENCIA  │  │  ADAPTACIÓN  │  │ RECUPERACIÓN│  │
│  │              │  │              │  │             │  │
│  │ • Circuit    │  │ • HPA/VPA    │  │ • Self-     │  │
│  │   Breaker    │  │ • Queue-based│  │   Healing   │  │
│  │ • Bulkhead   │  │   Leveling   │  │ • Auto      │  │
│  │ • Retry +    │  │ • Adaptive   │  │   Rollback  │  │
│  │   Backoff    │  │   Rate Limit │  │ • PITR      │  │
│  │ • Timeouts   │  │ • Priority   │  │ • Chaos     │  │
│  │ • Degradation│  │   Queues     │  │   Engineer. │  │
│  └──────────────┘  └──────────────┘  └─────────────┘  │
│                                                        │
└────────────────────────────────────────────────────────┘
        </pre>
    </div>

    <h3>11.5.1 RESISTENCIA - Tolerancia a Fallos Parciales</h3>

    <h4>A) Bulkhead Pattern - Aislamiento de Recursos</h4>
    
    <div class="diagram">
        <div class="diagram-title">Aislamiento de Recursos por Tipo de Operación</div>
        <pre>
┌────────────────────────────────────────────────────────┐
│           MS-PAGOS (Ejemplo)                           │
├────────────────────────────────────────────────────────┤
│                                                        │
│  ┌─────────────────────┐       ┌──────────────────┐   │
│  │  Thread Pool        │       │  Thread Pool     │   │
│  │  CONSULTAS          │       │  TRANSFERENCIAS  │   │
│  │                     │       │                  │   │
│  │  • 20 threads       │       │  • 10 threads    │   │
│  │  • Timeout: 5s      │       │  • Timeout: 30s  │   │
│  │  • Queue: 100       │       │  • Queue: 50     │   │
│  └──────────┬──────────┘       └─────────┬────────┘   │
│             │                            │             │
│             ▼                            ▼             │
│  ┌─────────────────────┐       ┌──────────────────┐   │
│  │  Connection Pool    │       │  Connection Pool │   │
│  │  READ REPLICA       │       │  PRIMARY DB      │   │
│  │  50 connections     │       │  20 connections  │   │
│  └─────────────────────┘       └──────────────────┘   │
│                                                        │
│  ┌───────────────────────────────────────────────┐    │
│  │  Thread Pool - NOTIFICACIONES                 │    │
│  │  • 5 threads                                  │    │
│  │  • No crítico, puede fallar sin afectar core  │    │
│  └───────────────────────────────────────────────┘    │
│                                                        │
└────────────────────────────────────────────────────────┘
        </pre>
    </div>

    <p><strong>Beneficio:</strong> Si notificaciones fallan o se saturan, NO afecta las transferencias. Aislamiento total de recursos críticos vs no-críticos.</p>

    <h3>11.5.2 ADAPTACIÓN - Escalabilidad Dinámica</h3>

    <h4>A) Queue-Based Load Leveling</h4>
    
    <div class="diagram">
        <div class="diagram-title">Absorción de Picos con Colas</div>
        <pre>
Tráfico Variable
     │
     │  Peak: 10,000 req/s
     │  ────────┐
     │         │ 
     │  Normal:│ 2,000 req/s
     │  ───────┼─────────
     │         │
     ▼         ▼
┌────────────────────┐
│   KAFKA QUEUE      │
│                    │
│  Buffer: 1M msgs   │
│  TTL: 10 minutes   │
└────────┬───────────┘
         │ Consume at sustainable rate
         │
         ▼
┌────────────────────┐
│  Workers Pool      │
│                    │
│  Process: 3K req/s │
│  Consistent rate   │
└────────────────────┘

Beneficio: Sistema procesa a ritmo constante
Ahorro: 50-60% en costos vs aprovisionar para pico
        </pre>
    </div>

    <h4>B) Rate Limiting Adaptativo</h4>
    
    <table>
        <thead>
            <tr>
                <th>Carga del Sistema</th>
                <th>Rate Limit Ajustado</th>
                <th>Acción</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Baja (&lt;30%)</td>
                <td>150 req/min (↑ 50%)</td>
                <td>Permitir más requests</td>
            </tr>
            <tr>
                <td>Normal (30-75%)</td>
                <td>100 req/min (base)</td>
                <td>Rate limit estándar</td>
            </tr>
            <tr>
                <td>Alta (75-90%)</td>
                <td>75 req/min (↓ 25%)</td>
                <td>Reducir carga</td>
            </tr>
            <tr>
                <td>Crítica (&gt;90%)</td>
                <td>50 req/min (↓ 50%)</td>
                <td>Proteger sistema</td>
            </tr>
        </tbody>
    </table>

    <h4>C) Priority Queues</h4>
    
    <table>
        <thead>
            <tr>
                <th>Prioridad</th>
                <th>Tipo de Operación</th>
                <th>Workers</th>
                <th>SLA</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>CRITICAL</strong></td>
                <td>Transferencias &gt;$10K, Nómina</td>
                <td>10</td>
                <td>&lt;5s</td>
            </tr>
            <tr>
                <td><strong>HIGH</strong></td>
                <td>Transferencias normales</td>
                <td>5</td>
                <td>&lt;30s</td>
            </tr>
            <tr>
                <td><strong>NORMAL</strong></td>
                <td>Consultas, notificaciones</td>
                <td>3</td>
                <td>&lt;2min</td>
            </tr>
            <tr>
                <td><strong>LOW</strong></td>
                <td>Reportes, analytics</td>
                <td>1</td>
                <td>Best-effort</td>
            </tr>
        </tbody>
    </table>

    <h3>11.5.3 RECUPERACIÓN - Auto-Healing</h3>

    <h4>A) Self-Healing en Kubernetes</h4>
    
    <ol>
        <li><strong>Liveness Probe Falla:</strong> K8s mata y recrea el pod automáticamente (&lt;30s)</li>
        <li><strong>Readiness Probe Falla:</strong> Pod removido del balanceador hasta recuperarse</li>
        <li><strong>Node Falla:</strong> K8s reschedula todos los pods en otros nodos (&lt;5min)</li>
        <li><strong>OOM (Out of Memory):</strong> VPA ajusta límites y recrea con más memoria</li>
    </ol>

    <h4>B) Automated Rollback</h4>
    
    <table>
        <thead>
            <tr>
                <th>Métrica</th>
                <th>Threshold</th>
                <th>Acción</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Error rate</td>
                <td>+50% vs baseline</td>
                <td>Rollback inmediato (&lt;60s)</td>
            </tr>
            <tr>
                <td>Latencia p95</td>
                <td>+100ms vs baseline</td>
                <td>Rollback inmediato</td>
            </tr>
            <tr>
                <td>CPU usage</td>
                <td>&gt;90% sostenido</td>
                <td>Rollback en 2 min</td>
            </tr>
            <tr>
                <td>5xx errors</td>
                <td>&gt;10 en 1 minuto</td>
                <td>Rollback inmediato</td>
            </tr>
        </tbody>
    </table>

    <h4>C) Chaos Engineering</h4>
    
    <ul>
        <li><strong>Game Days (Mensual):</strong> Simular fallo de nodo, inyectar latencia, validar auto-recuperación</li>
        <li><strong>Chaos Monkey (Producción):</strong> Mata 1 pod aleatorio cada hora (horario no-peak 2am-6am)</li>
        <li><strong>Latency Monkey:</strong> Inyecta 200-500ms latencia en 5% de requests aleatoriamente</li>
        <li><strong>Failure Injection (Staging):</strong> Simular caída de Redis, Kafka, ACH network</li>
    </ul>

    <h3>11.5.4 Métricas de Resiliencia</h3>
    
    <table>
        <thead>
            <tr>
                <th>Métrica</th>
                <th>Objetivo</th>
                <th>Alertas</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>MTBF</strong> (Mean Time Between Failures)</td>
                <td>&gt;720 horas (30 días)</td>
                <td>&lt;480 horas</td>
            </tr>
            <tr>
                <td><strong>MTTR</strong> (Mean Time To Recover)</td>
                <td>&lt;15 minutos</td>
                <td>&gt;30 minutos</td>
            </tr>
            <tr>
                <td><strong>Uptime</strong></td>
                <td>&gt;99.9%</td>
                <td>&lt;99.5%</td>
            </tr>
            <tr>
                <td><strong>Auto-Healing Events</strong></td>
                <td>&lt;10/día</td>
                <td>&gt;50/día</td>
            </tr>
            <tr>
                <td><strong>Rollback Rate</strong></td>
                <td>&lt;5% de deploys</td>
                <td>&gt;10%</td>
            </tr>
            <tr>
                <td><strong>Data Loss</strong></td>
                <td>0 bytes</td>
                <td>Cualquier pérdida</td>
            </tr>
        </tbody>
    </table>

    <div class="success">
        <strong>Resultado Esperado:</strong><br>
        Un sistema bancario que <strong>nunca falla completamente</strong>, se <strong>adapta a la demanda</strong> automáticamente y se <strong>auto-recupera de fallos</strong> en segundos, manteniendo 99.9% uptime garantizado.
    </div>
</div>

<div class="footer">
    <p>Propuesta de Arquitectura - Sistema Bancario BP | Capítulo 11 de 17</p>
</div>

</body>
</html>